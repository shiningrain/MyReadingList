# AI model training Reading List

‚õîÔ∏è `Pending` üïê `Updating` üëç `Finished` ‚≠ê `1-5 Stars`

**Description:** xxx.

**Keywords: AI Model training;**

1.  üëç **Auto-Keras: An Efficient Neural Architecture Search System**
    [[Notes](./notes/jin2019auto.md)]
    [[Paper](https://dl.acm.org/doi/abs/10.1145/3292500.3330648)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** A efficient NAS search for model structure under keras/tensorflow framework.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{jin2019auto,
    title={Auto-keras: An efficient neural architecture search system},
    author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
    booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
    pages={1946--1956},
    year={2019}
    }
    ```

    #### URL

    ```
    Paper: https://dl.acm.org/doi/abs/10.1145/3292500.3330648
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:z_Md7qY8rZkJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KqJ0Pg:AAGBfm0AAAAAYEWMyPjveCX9qtbazfhiyKx6bSSMKIcC&scisig=AAGBfm0AAAAAYEWMyNS9sB6x9lSYU1L_pjfC1X1s_Q00&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>


2.  üëç **HyperDrive: Exploring Hyperparameters with POP Scheduling**
    [[Notes](./notes/rasley2017hyperdrive.md)]
    [[Paper](https://dl.acm.org/doi/pdf/10.1145/3135974.3135994)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** A efficient NAS search for model structure under keras/tensorflow framework.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{rasley2017hyperdrive,
    title={Hyperdrive: Exploring hyperparameters with pop scheduling},
    author={Rasley, Jeff and He, Yuxiong and Yan, Feng and Ruwase, Olatunji and Fonseca, Rodrigo},
    booktitle={Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference},
    pages={1--13},
    year={2017}
    }
    ```

    #### URL

    ```
    Paper: https://dl.acm.org/doi/pdf/10.1145/3135974.3135994
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:259sCLXqS-EJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KqXHnQ:AAGBfm0AAAAAYEWSBnTaOh97MsUR3AsvnZtmfRGUsd_F&scisig=AAGBfm0AAAAAYEWSBkKNkDpxWIpK7p_oiCwu1BHf_HjZ&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

3.  üëç **GPflowOpt: A Bayesian Optimization Library using TensorFlow**
    [[Notes](./notes/knudde2017gpflowopt.md)]
    [[Paper](https://arxiv.org/abs/1711.03845)]
    ‚≠ê‚≠ê

    **Overview:** Bayesian optimization based on GPflow lib and tensorflow lib.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{knudde2017gpflowopt,
    title={GPflowOpt: A Bayesian optimization library using TensorFlow},
    author={Knudde, Nicolas and van der Herten, Joachim and Dhaene, Tom and Couckuyt, Ivo},
    journal={arXiv preprint arXiv:1711.03845},
    year={2017}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/abs/1711.03845
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:qzmV60Sn0HgJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6Knc60Q:AAGBfm0AAAAAYEbZ80QjpREdiJRP6WOzpfFLK2HFPhvK&scisig=AAGBfm0AAAAAYEbZ80vgcxKvl4CQmOsCPfmc-ybniN6f&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

4.  üëç **Speeding up Hyper-parameter Optimization by Extrapolation of Learning Curves using Previous Builds**
    [[Notes](./notes/chandrashekaran2017speeding.md)]
    [[Paper](http://ecmlpkdd2017.ijs.si/papers/paperID653.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a usage of simple regression based extraplation model to predict the future training curve.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{chandrashekaran2017speeding,
    title={Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds},
    author={Chandrashekaran, Akshay and Lane, Ian R},
    booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
    pages={477--492},
    year={2017},
    organization={Springer}
    }
    ```

    #### URL

    ```
    Paper: http://ecmlpkdd2017.ijs.si/papers/paperID653.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:xdirqwHfRBQJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6Kgi0dk:AAGBfm0AAAAAYEcnydn8FAfPU3dXWg9C-pSXO008MAFO&scisig=AAGBfm0AAAAAYEcnya0oWp36R6Baz1eD283ze7COIZ0l&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

5.  üëç **Population Based Training of Neural Networks**
    [[Notes](./notes/jaderbergpopulation.md)]
    [[Paper](https://arxiv.org/abs/1711.09846)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a population based training using a asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance on **deep reinforcement learning**.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{jaderbergpopulation,
    title={Population Based Training of Neural Networks (PBT)},
    author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/abs/1711.09846
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:uKQjJoB-8k0J:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KbGDkk:AAGBfm0AAAAAYEnDFknFEw3JYIjmxet8U-42yGp90qJx&scisig=AAGBfm0AAAAAYEnDFhaDC1ktMXiZwAW8v2ATYBrekBDS&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

    
6.  üëç **Hippo: Taming Hyper-parameter Optimization of Deep Learning with Stage Trees**
    [[Notes](./notes/jaderbergpopulation.md)]
    [[Paper](https://arxiv.org/pdf/2006.11972.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a hyperparams optimization system with stage-tree implemented on distributed GPU server.
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{shin2020hippo,
    title={Hippo: Taming Hyper-parameter Optimization of Deep Learning with Stage Trees},
    author={Shin, Ahnjae and Kim, Do Yoon and Jeong, Joo Seong and Chun, Byung-Gon},
    journal={arXiv preprint arXiv:2006.11972},
    year={2020}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/2006.11972.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:Pn9PS8VmpoUJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KbeJrA:AAGBfm0AAAAAYEnbPrC_tKNuDkAEBYEW9deoIiE_whAb&scisig=AAGBfm0AAAAAYEnbPuK78WLm59uW1ItLui7-rVDQa7XY&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

7.  üëç **Combination of Hyperband and Bayesian Optimization for Hyperparameter Optimization in Deep Learning**
    [[Notes](./notes/wang2018combination.md)]
    [[Paper](https://arxiv.org/abs/1801.01596)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a method that combine the bayesian with Hyperband algorithm to adjust DL model hyperparameters.
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{wang2018combination,
    title={Combination of hyperband and Bayesian optimization for hyperparameter optimization in deep learning},
    author={Wang, Jiazhuo and Xu, Jason and Wang, Xuejun},
    journal={arXiv preprint arXiv:1801.01596},
    year={2018}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/abs/1801.01596
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:rF4pLmZ1DDUJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6LKeIU0:AAGBfm0AAAAAYF2bOU2OPJBkFB4E3Gp5sscantpO8E5x&scisig=AAGBfm0AAAAAYF2bOQMreroPFcI8C4eJ7Y1y-Ur2yojP&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

8. üëç **Algorithms for Hyper-Parameter Optimization**
    [[Notes](./notes/bergstra2011algorithms.md)]
    [[Paper](https://hal.inria.fr/hal-00642998/)]
    ‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** Propose 2 classical hyperparameters tuning method: Gaussian Process and TPE algorithm.
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{bergstra2011algorithms,
    title={Algorithms for hyper-parameter optimization},
    author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
    booktitle={25th annual conference on neural information processing systems (NIPS 2011)},
    volume={24},
    year={2011},
    organization={Neural Information Processing Systems Foundation}
    }
    ```

    #### URL

    ```
    Paper: https://hal.inria.fr/hal-00642998/
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:OSYktxdmDR8J:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6EzA6_A:AAGBfm0AAAAAYKPF8_C5BsRfJmou80UHnk5Z2LI5vN_4&scisig=AAGBfm0AAAAAYKPF89QUvo6lXDSp2mq18OLCKvYSVDUp&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>


9. üëç **PRACTICAL BAYESIAN OPTIMIZATION OF MACHINE LEARNING ALGORITHMS**
    [[Notes](./notes/snoek2012practical.md)]
    [[Paper](https://arxiv.org/pdf/1206.2944.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Propose new algorithms that can run good results with multiple cores for parallel experimentation in the bayes optimization frameworks.
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{snoek2012practical,
    title={Practical bayesian optimization of machine learning algorithms},
    author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
    journal={arXiv preprint arXiv:1206.2944},
    year={2012}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1206.2944.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:OSYktxdmDR8J:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6EzA6_A:AAGBfm0AAAAAYKPF8_C5BsRfJmou80UHnk5Z2LI5vN_4&scisig=AAGBfm0AAAAAYKPF89QUvo6lXDSp2mq18OLCKvYSVDUp&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

10. üëç **Evolutionary Optimization of Hyperparameters in Deep Learning Models**
    [[Notes](./notes/kim2019evolutionary.md)]
    [[Paper](https://ieeexplore.ieee.org/abstract/document/8790354)]
    ‚≠ê‚≠ê

    **Overview:** Propose a genetic method to find optimal fucntions and activations..
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{kim2019evolutionary,
    title={Evolutionary optimization of hyperparameters in deep learning models},
    author={Kim, Jin-Young and Cho, Sung-Bae},
    booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)},
    pages={831--837},
    year={2019},
    organization={IEEE}
    }
    ```

    #### URL

    ```
    Paper: https://ieeexplore.ieee.org/abstract/document/8790354
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:TfxTq6aVumkJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6EoQH5g:AAGBfm0AAAAAYKUVB5jJFfqXxelENdzOXlpGL_rLWtDh&scisig=AAGBfm0AAAAAYKUVByIXXxiSUtJ2uci-flA12uSjpKJm&scisf=4&ct=citation&cd=0&hl=zh-CN&scfhb=1
    ```

    </details>

11. üëç **Gradient-based Hyperparameter Optimization through Reversible Learning**
    [[Notes](./notes/maclaurin2015gradient.md)]
    [[Paper](http://proceedings.mlr.press/v37/maclaurin15.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Propose an algorithm that exactly reverses stochastic gradient descent with momentum to compute gradients to all hyperparameters..
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{maclaurin2015gradient,
    title={Gradient-based hyperparameter optimization through reversible learning},
    author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
    booktitle={International conference on machine learning},
    pages={2113--2122},
    year={2015},
    organization={PMLR}
    }
    ```

    #### URL

    ```
    Paper: http://proceedings.mlr.press/v37/maclaurin15.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:oK1ZvNVI-uMJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6EoQ8_s:AAGBfm0AAAAAYKUV6_se36b1b_RsPTPRnV5A0aH5J0CT&scisig=AAGBfm0AAAAAYKUV63VPI_YU7HBEZsFtAvbUISilTUoM&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>