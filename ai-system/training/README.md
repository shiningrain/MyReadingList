# AI model training Reading List

‚õîÔ∏è `Pending` üïê `Updating` üëç `Finished` ‚≠ê `1-5 Stars`

**Description:** xxx.

**Keywords: AI Model training;**

1.  üëç **Auto-Keras: An Efficient Neural Architecture Search System**
    [[Notes](./notes/jin2019auto.md)]
    [[Paper](https://dl.acm.org/doi/abs/10.1145/3292500.3330648)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** A efficient NAS search for model structure under keras/tensorflow framework.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{jin2019auto,
    title={Auto-keras: An efficient neural architecture search system},
    author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
    booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
    pages={1946--1956},
    year={2019}
    }
    ```

    #### URL

    ```
    Paper: https://dl.acm.org/doi/abs/10.1145/3292500.3330648
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:z_Md7qY8rZkJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KqJ0Pg:AAGBfm0AAAAAYEWMyPjveCX9qtbazfhiyKx6bSSMKIcC&scisig=AAGBfm0AAAAAYEWMyNS9sB6x9lSYU1L_pjfC1X1s_Q00&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>


2.  üëç **HyperDrive: Exploring Hyperparameters with POP Scheduling**
    [[Notes](./notes/rasley2017hyperdrive.md)]
    [[Paper](https://dl.acm.org/doi/pdf/10.1145/3135974.3135994)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** A efficient NAS search for model structure under keras/tensorflow framework.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{rasley2017hyperdrive,
    title={Hyperdrive: Exploring hyperparameters with pop scheduling},
    author={Rasley, Jeff and He, Yuxiong and Yan, Feng and Ruwase, Olatunji and Fonseca, Rodrigo},
    booktitle={Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference},
    pages={1--13},
    year={2017}
    }
    ```

    #### URL

    ```
    Paper: https://dl.acm.org/doi/pdf/10.1145/3135974.3135994
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:259sCLXqS-EJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KqXHnQ:AAGBfm0AAAAAYEWSBnTaOh97MsUR3AsvnZtmfRGUsd_F&scisig=AAGBfm0AAAAAYEWSBkKNkDpxWIpK7p_oiCwu1BHf_HjZ&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

3.  üëç **GPflowOpt: A Bayesian Optimization Library using TensorFlow**
    [[Notes](./notes/knudde2017gpflowopt.md)]
    [[Paper](https://arxiv.org/abs/1711.03845)]
    ‚≠ê‚≠ê

    **Overview:** Bayesian optimization based on GPflow lib and tensorflow lib.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{knudde2017gpflowopt,
    title={GPflowOpt: A Bayesian optimization library using TensorFlow},
    author={Knudde, Nicolas and van der Herten, Joachim and Dhaene, Tom and Couckuyt, Ivo},
    journal={arXiv preprint arXiv:1711.03845},
    year={2017}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/abs/1711.03845
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:qzmV60Sn0HgJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6Knc60Q:AAGBfm0AAAAAYEbZ80QjpREdiJRP6WOzpfFLK2HFPhvK&scisig=AAGBfm0AAAAAYEbZ80vgcxKvl4CQmOsCPfmc-ybniN6f&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

4.  üëç **Speeding up Hyper-parameter Optimization by Extrapolation of Learning Curves using Previous Builds**
    [[Notes](./notes/chandrashekaran2017speeding.md)]
    [[Paper](http://ecmlpkdd2017.ijs.si/papers/paperID653.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a usage of simple regression based extraplation model to predict the future training curve.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{chandrashekaran2017speeding,
    title={Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds},
    author={Chandrashekaran, Akshay and Lane, Ian R},
    booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
    pages={477--492},
    year={2017},
    organization={Springer}
    }
    ```

    #### URL

    ```
    Paper: http://ecmlpkdd2017.ijs.si/papers/paperID653.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:xdirqwHfRBQJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6Kgi0dk:AAGBfm0AAAAAYEcnydn8FAfPU3dXWg9C-pSXO008MAFO&scisig=AAGBfm0AAAAAYEcnya0oWp36R6Baz1eD283ze7COIZ0l&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

5.  üëç **Population Based Training of Neural Networks**
    [[Notes](./notes/jaderbergpopulation.md)]
    [[Paper](https://arxiv.org/abs/1711.09846)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a population based training using a asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance on **deep reinforcement learning**.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{jaderbergpopulation,
    title={Population Based Training of Neural Networks (PBT)},
    author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/abs/1711.09846
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:uKQjJoB-8k0J:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KbGDkk:AAGBfm0AAAAAYEnDFknFEw3JYIjmxet8U-42yGp90qJx&scisig=AAGBfm0AAAAAYEnDFhaDC1ktMXiZwAW8v2ATYBrekBDS&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>

    
6.  üëç **Hippo: Taming Hyper-parameter Optimization of Deep Learning with Stage Trees**
    [[Notes](./notes/jaderbergpopulation.md)]
    [[Paper](https://arxiv.org/pdf/2006.11972.pdf)]
    ‚≠ê‚≠ê

    **Overview:** Proposed a hyperparams optimization system with stage-tree implemented on distributed GPU server.
    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{shin2020hippo,
    title={Hippo: Taming Hyper-parameter Optimization of Deep Learning with Stage Trees},
    author={Shin, Ahnjae and Kim, Do Yoon and Jeong, Joo Seong and Chun, Byung-Gon},
    journal={arXiv preprint arXiv:2006.11972},
    year={2020}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/2006.11972.pdf
    Citation: https://scholar.googleusercontent.com/scholar.bib?q=info:Pn9PS8VmpoUJ:scholar.google.com/&output=citation&scisdr=CgVZZtBEEKrl6KbeJrA:AAGBfm0AAAAAYEnbPrC_tKNuDkAEBYEW9deoIiE_whAb&scisig=AAGBfm0AAAAAYEnbPuK78WLm59uW1ItLui7-rVDQa7XY&scisf=4&ct=citation&cd=0&hl=zh-CN
    ```

    </details>